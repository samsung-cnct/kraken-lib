---
- name: Set cluster fact
  set_fact:
    cluster: "{{ a_cluster }}"

- name: Set facts needed for service management
  set_fact:
    cluster_services: "{{ cluster.helmConfig.charts | default([]) }}"
    cluster_repos: "{{ cluster.helmConfig.repos | default([]) }}"
    helm_command: "{{ helm_commands[cluster.name] }}"
    helm_home: "{{ config_base }}/{{ cluster.name }}/.helm"
    kubeconfig: "{{ config_base }}/{{ cluster.name }}/admin.kubeconfig"
    kubectl: "{{ kubectl_commands[cluster.name] }}"
    cluster_namespaces: []

- name: Set dependent facts
  set_fact:
    helm_path: "{{ helm_command | dirname }}"


- name: Build a list of namespaces used for services
  set_fact:
    cluster_namespaces: "{{ cluster_namespaces + [item.namespace | default('kube-system')] }}"
  with_items: "{{ cluster_services }}"

- name: Filter the list of namespaces
  set_fact:
    cluster_namespaces: "{{ cluster_namespaces | unique | reject('search', 'kube-system') | list }}"

- name: See if tiller rc if present
  command: >
      {{ kubectl }} --kubeconfig={{ kubeconfig }} get deployment {{ tiller }} --namespace=kube-system
  register: tiller_present
  when: kubeconfig | is_file
  ignore_errors: yes
  failed_when: false

- name: Clean up releases
  command: >
    {{ helm_command }} delete --purge {{ item.name }}
  environment:
    KUBECONFIG: "{{ kubeconfig }}"
    HELM_HOME: "{{ helm_home }}"
    PATH: "{{ helm_path }}:{{ ansible_env.PATH }}"
  with_items: "{{ cluster_services }}"
  ignore_errors: yes
  when:
    - not (tiller_present | skipped)
    - tiller_present.stderr.find("Error") != -1

- name: Clean up tiller if present
  command: >
    {{ kubectl }} --kubeconfig={{ kubeconfig }} delete deployment {{ tiller }} --namespace=kube-system
  when:
    - not (tiller_present | skipped)
    - tiller_present.stderr.find("Error") != -1
  ignore_errors: yes

- name: Collect all services
  command: >
    {{ kubectl }} --kubeconfig={{ kubeconfig }} get services --all-namespaces -o json
  register: added_services
  when: kraken_action == 'down' and kubeconfig | is_file
  ignore_errors: yes

- name: Register services fact
  set_fact:
    added_services_map: "{{ added_services.stdout | from_json }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Set services info
  set_fact:
    the_services: "{{ added_services_map['items'] }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Set load balanced service data
  set_fact:
    load_balanced_services: "{{ the_services|json_query('[?status.loadBalancer.ingress[0].hostname != null].{namespace: metadata.namespace, name: metadata.name}') }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Clean up services
  command: >
    {{ kubectl }} --kubeconfig={{ kubeconfig }} delete --namespace {{ item.namespace }} svc {{ item.name }}
  with_items: "{{ load_balanced_services }}"
  when: kraken_action == 'down'
  ignore_errors: yes

- name: Delete all service namespaces
  command: >
    {{ kubectl }} --kubeconfig={{ kubeconfig }} delete namespace {{ item }}
  with_items: "{{ cluster_namespaces }}"
  when: cluster_namespaces is defined
  ignore_errors: yes

- name: Get vpc id
  shell: "terraform state show -state={{ config_base }}/{{ cluster.name }}/terraform.tfstate module.vpc.aws_vpc.vpc  | awk '/^id/{print $3}'"
  register: terraform_state_show
  when: kraken_action == 'down'
  changed_when: false

- name: Set vpc_id fact
  set_fact:
    vpcid: "{{ terraform_state_show.stdout }}"
  when: kraken_action == 'down'

# because BOTO2 doesn't support AWS_SHARED_CREDENTIALS_FILE environment variable, do some mangling
- include: aws_config.yaml
  when: cluster.providerConfig.provider == 'aws'

- name: Wait for ELBs to be deleted
  action:
    module: ec2_elb_facts
    region: "{{ cluster.providerConfig.region }}"
    aws_access_key: "{{ cluster.providerConfig.authentication.accessKey or omit }}"
    aws_secret_key: "{{ cluster.providerConfig.authentication.accessSecret or omit }}"
    security_token: "{{ cluster.providerConfig.authentication.securityToken | default(omit) }}"
    profile: "{{ cluster.providerConfig.authentication.credentialsProfile or omit }}"
  register: elb_facts
  vars:
    vpc_lookup: "elbs[?vpc_id=='{{ vpcid }}']"
  when: kraken_action == 'down' and cluster.providerConfig.provider == 'aws' and cluster.providerConfig.existing_vpc is not defined
  until: (elb_facts is none) or (elb_facts | json_query(vpc_lookup) is none) or (elb_facts | json_query(vpc_lookup) | length <= 1)
  retries: 120
  delay: 5
